[[_architecture]]
= Architecture

image::architecture.svg[]

{project-title} processes data in batch fashion: a fixed number of records (batch AKA chunk) is read, processed, and written at a time.
Then the cycle is repeated until there's no more data on the source.

[[_architecture_keys]]
== Keys
Import commands (<<_file_import,`file-import`>>, <<_db_import,`db-import`>>, <<_faker_import,`faker-import`>>) construct keys from input records by concatenating a keyspace prefix and fields.

image::mapping.png[]

[[_architecture_batch]]
== Batching

The default batch size is 50, which means that an execution step reads 50 items at a time from the source, processes them, and finally writes then to the target.
If the target is Redis, writing is done in a single command {link_redis_pipeline} to minimize the number of roundtrips to the server.

You can change the batch size (and hence pipeline size) using the `--batch` option.
The optimal batch size in terms of throughput depends on many factors like record size and command types (see {link_pipeline_tuning} for details).

.Batching example
[source]
----
include::{testdir}/faker-import-tsadd[]
----

[[_architecture_threads]]
== Multi-threading

It is possible to parallelize processing by using multiple threads.
In that configuration, each chunk of items is read, processed, and written in a separate thread of execution.
This is different from partitioning where items would be read by multiple readers.
Here, only one reader is being accessed from multiple threads.

To set the number of threads use the `--threads` option.

.Multi-threading example
[source]
----
include::{testdir}/db-import-postgresql-multithreaded[]
----

[[_architecture_processing]]
== Processing

Processors are applied to records in the following order:

* Transforms
* Regular expressions
* Filters

[[_architecture_transforms]]
=== Transforms

Transforms allow you to create/update/delete fields using the {link_spel} (SpEL):

* `field1='foo'` -> generate a field named `field1` containing the string `foo`
* `temp=(temp-32)*5/9` -> convert temperature from Fahrenheit to Celsius
* `name=remove(first).concat(remove(last))` -> concatenate `first` and `last` fields and delete them
* `field2=null` -> delete `field2`

Input fields are accessed by name (e.g. `field3=field1+field2`).

The transform processor also exposes functions and variables that can be accessed using the `#` prefix:

* `date`: Date parser/formatter ({link_java_dateformat})
* `geo`: Convenience method that takes a longitude and a latitude to produce a RediSearch geo-location string in the form `longitude,latitude`
* `index`: Sequence number of the item being generated
* `redis`: Handle to invoke Redis commands ({link_lettuce_api})

.Processor Example
[source]
----
riot file-import --process epoch="#date.parse(mydate).getTime()" location="#geo(lon,lat)" id="#index" name="#redis.hget('person1','lastName')" ...
----

[[_architecture_regex]]
=== Regular Expressions

Extract patterns from source fields using regular expressions:
[source]
----
riot file-import --regex name="(?<first>\w+)\/(?<last>\w+)" ...
----

[[_architecture_filters]]
=== Filters

Filters allow you to exclude records that don't match a SpEL boolean expression.

For example this filter will only keep records where the `value` field is a series of digits:

[source]
----
riot file-import --filter "value matches '\\d+'" ...
----